{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import class2one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(root, subset) -> list[tuple[Path, Path]]:\n",
    "    assert subset in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    root = Path(root)\n",
    "\n",
    "    img_path = root / subset / \"img\"\n",
    "    full_path = root / subset / \"gt\"\n",
    "\n",
    "    images = sorted(img_path.glob(\"*.png\"))\n",
    "    full_labels = sorted(full_path.glob(\"*.png\"))\n",
    "\n",
    "    return list(zip(images, full_labels))\n",
    "\n",
    "class SliceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset,\n",
    "        root_dir,\n",
    "        img_transform=None,\n",
    "        gt_transform=None,\n",
    "        augment=False,\n",
    "        equalize=False,\n",
    "        debug=False,\n",
    "    ):\n",
    "        self.root_dir: str = root_dir\n",
    "        self.img_transform: Callable = img_transform\n",
    "        self.gt_transform: Callable = gt_transform\n",
    "        self.augmentation: bool = augment  # TODO: implement\n",
    "        self.equalize: bool = equalize  # TODO: know if we need it\n",
    "\n",
    "        # TODO make our own test set, now 5453 train and 1967 val\n",
    "        self.files = make_dataset(root_dir, subset)\n",
    "        if debug:\n",
    "            self.files = self.files[:10]\n",
    "\n",
    "        print(f\">> Created {subset} dataset with {len(self)} images...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index) -> dict[str, Union[Tensor, int, str]]:\n",
    "        img_path, gt_path = self.files[index]\n",
    "\n",
    "        img: Tensor = self.img_transform(Image.open(img_path))\n",
    "        gt: Tensor = self.gt_transform(Image.open(gt_path))\n",
    "\n",
    "        if self.augmentation:\n",
    "\n",
    "            # Apply augmentation if random is above trheshold\n",
    "            if random.random() > 0.0:\n",
    "\n",
    "                a = None\n",
    "\n",
    "                # Only apply one augmentation at a time\n",
    "                random_val = random.random()\n",
    "\n",
    "                if random_val < 0.2:\n",
    "                    a = \"flip\"\n",
    "                    img = transforms.functional.vflip(img)\n",
    "                    gt = transforms.functional.vflip(gt)\n",
    "\n",
    "                elif random_val < 0.4:\n",
    "                    a = \"rotate\"\n",
    "                    angle = random.uniform(-5, 5)\n",
    "                    img = transforms.functional.rotate(img, angle)\n",
    "                    gt = transforms.functional.rotate(gt, angle)\n",
    "\n",
    "                elif random_val < 0.7:\n",
    "                    a = \"crop\"\n",
    "                    # Custom cropping implementation\n",
    "                    crop_size = (int(img.size(1) / 1.1), int(img.size(2) / 1.1))\n",
    "                    i = random.randint(0, img.size(1) - crop_size[0])\n",
    "                    j = random.randint(0, img.size(2) - crop_size[1])\n",
    "                    img = img[:, i:i+crop_size[0], j:j+crop_size[1]]\n",
    "                    gt = gt[:, i:i+crop_size[0], j:j+crop_size[1]]\n",
    "                    \n",
    "                else:\n",
    "                    a = \"noise\"\n",
    "                    noise = torch.randn(img.size()) * 0.01\n",
    "                    img = img + noise\n",
    "\n",
    "        _, W, H = img.shape\n",
    "        K, _, _ = gt.shape\n",
    "        assert gt.shape == (K, W, H)\n",
    "\n",
    "        return {\"images\": img, \"gts\": gt, \"stems\": img_path.stem, \"original_image\": self.img_transform(Image.open(img_path)), \"original_gt\": self.gt_transform(Image.open(gt_path)), \"a\": a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Created train dataset with 5453 images...\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"../\" / Path(\"data\") / \"SEGTHOR\"\n",
    "\n",
    "K = 5  # Number of classes\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        lambda img: img.convert(\"L\"),  # convert to grayscale\n",
    "        lambda img: np.array(img)[np.newaxis, ...],\n",
    "        lambda nd: nd / 255,  # max <= 1 (range [0, 1])\n",
    "        lambda nd: torch.tensor(nd, dtype=torch.float32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "gt_transform = transforms.Compose(\n",
    "    [\n",
    "        lambda img: np.array(img)[...],\n",
    "        # The idea is that the classes are mapped to {0, 255} for binary cases\n",
    "        # {0, 85, 170, 255} for 4 classes\n",
    "        # {0, 51, 102, 153, 204, 255} for 6 classes\n",
    "        # Very sketchy but that works here and that simplifies visualization\n",
    "        lambda nd: nd / (255 / (K - 1)) if K != 5 else nd / 63,  # max <= 1\n",
    "        lambda nd: torch.tensor(nd, dtype=torch.int64)[\n",
    "            None, ...\n",
    "        ],  # Add one dimension to simulate batch\n",
    "        lambda t: class2one_hot(t, K=K),\n",
    "        itemgetter(0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = SliceDataset(\n",
    "    \"train\",\n",
    "    root_dir,\n",
    "    img_transform=img_transform,\n",
    "    gt_transform=gt_transform,\n",
    "    debug=False,\n",
    "    augment=True,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=1, num_workers=4, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "fig, ax = plt.subplots(N, 4, figsize=(12, N*3))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    if idx == N:\n",
    "        break\n",
    "    # title for this idx\n",
    "    ax[idx, 0].annotate(batch[\"a\"][0], xy=(0, 1.1),\n",
    "            xycoords=\"axes fraction\",\n",
    "            fontsize=12,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "    ax[idx, 0].imshow(batch[\"original_image\"].squeeze(), cmap=\"gray\")\n",
    "    color_map = plt.get_cmap(\"viridis\", K)\n",
    "    gt_colored = color_map(batch[\"original_gt\"][0].argmax(dim=0).cpu().numpy())\n",
    "    ax[idx, 1].imshow(gt_colored)\n",
    "    ax[idx, 2].imshow(batch[\"images\"].squeeze(), cmap=\"gray\")\n",
    "    color_map = plt.get_cmap(\"viridis\", K)\n",
    "    gt_colored = color_map(batch[\"gts\"][0].argmax(dim=0).cpu().numpy())\n",
    "    ax[idx, 3].imshow(gt_colored, cmap=\"gray\")\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[idx, i].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
