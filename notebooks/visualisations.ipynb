{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import class2one_hot\n",
    "sys.path.append('..')\n",
    "from data_augmenter import CTImageDataset, load_ct_images_and_gts, plot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/xwlg3vsx3sx5sfdlpbt58jg40000gn/T/ipykernel_80515/2589573767.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../bestweights_samed_512_r6_augment_no_normalize_yes.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fa_ti_q_proj_A.weight', 'fa_ti_q_proj_B.weight', 'fa_ti_v_proj_A.weight', 'fa_ti_v_proj_B.weight', 'sam.image_encoder.pos_embed', 'sam.image_encoder.patch_embed.proj.weight', 'sam.image_encoder.patch_embed.proj.bias', 'sam.image_encoder.blocks.0.norm1.weight', 'sam.image_encoder.blocks.0.norm1.bias', 'sam.image_encoder.blocks.0.attn.rel_pos_h', 'sam.image_encoder.blocks.0.attn.rel_pos_w', 'sam.image_encoder.blocks.0.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.0.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.0.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.0.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.0.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.0.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.0.attn.proj.weight', 'sam.image_encoder.blocks.0.attn.proj.bias', 'sam.image_encoder.blocks.0.norm2.weight', 'sam.image_encoder.blocks.0.norm2.bias', 'sam.image_encoder.blocks.0.mlp.lin1.weight', 'sam.image_encoder.blocks.0.mlp.lin1.bias', 'sam.image_encoder.blocks.0.mlp.lin2.weight', 'sam.image_encoder.blocks.0.mlp.lin2.bias', 'sam.image_encoder.blocks.1.norm1.weight', 'sam.image_encoder.blocks.1.norm1.bias', 'sam.image_encoder.blocks.1.attn.rel_pos_h', 'sam.image_encoder.blocks.1.attn.rel_pos_w', 'sam.image_encoder.blocks.1.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.1.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.1.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.1.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.1.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.1.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.1.attn.proj.weight', 'sam.image_encoder.blocks.1.attn.proj.bias', 'sam.image_encoder.blocks.1.norm2.weight', 'sam.image_encoder.blocks.1.norm2.bias', 'sam.image_encoder.blocks.1.mlp.lin1.weight', 'sam.image_encoder.blocks.1.mlp.lin1.bias', 'sam.image_encoder.blocks.1.mlp.lin2.weight', 'sam.image_encoder.blocks.1.mlp.lin2.bias', 'sam.image_encoder.blocks.2.norm1.weight', 'sam.image_encoder.blocks.2.norm1.bias', 'sam.image_encoder.blocks.2.attn.rel_pos_h', 'sam.image_encoder.blocks.2.attn.rel_pos_w', 'sam.image_encoder.blocks.2.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.2.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.2.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.2.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.2.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.2.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.2.attn.proj.weight', 'sam.image_encoder.blocks.2.attn.proj.bias', 'sam.image_encoder.blocks.2.norm2.weight', 'sam.image_encoder.blocks.2.norm2.bias', 'sam.image_encoder.blocks.2.mlp.lin1.weight', 'sam.image_encoder.blocks.2.mlp.lin1.bias', 'sam.image_encoder.blocks.2.mlp.lin2.weight', 'sam.image_encoder.blocks.2.mlp.lin2.bias', 'sam.image_encoder.blocks.3.norm1.weight', 'sam.image_encoder.blocks.3.norm1.bias', 'sam.image_encoder.blocks.3.attn.rel_pos_h', 'sam.image_encoder.blocks.3.attn.rel_pos_w', 'sam.image_encoder.blocks.3.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.3.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.3.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.3.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.3.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.3.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.3.attn.proj.weight', 'sam.image_encoder.blocks.3.attn.proj.bias', 'sam.image_encoder.blocks.3.norm2.weight', 'sam.image_encoder.blocks.3.norm2.bias', 'sam.image_encoder.blocks.3.mlp.lin1.weight', 'sam.image_encoder.blocks.3.mlp.lin1.bias', 'sam.image_encoder.blocks.3.mlp.lin2.weight', 'sam.image_encoder.blocks.3.mlp.lin2.bias', 'sam.image_encoder.blocks.4.norm1.weight', 'sam.image_encoder.blocks.4.norm1.bias', 'sam.image_encoder.blocks.4.attn.rel_pos_h', 'sam.image_encoder.blocks.4.attn.rel_pos_w', 'sam.image_encoder.blocks.4.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.4.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.4.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.4.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.4.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.4.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.4.attn.proj.weight', 'sam.image_encoder.blocks.4.attn.proj.bias', 'sam.image_encoder.blocks.4.norm2.weight', 'sam.image_encoder.blocks.4.norm2.bias', 'sam.image_encoder.blocks.4.mlp.lin1.weight', 'sam.image_encoder.blocks.4.mlp.lin1.bias', 'sam.image_encoder.blocks.4.mlp.lin2.weight', 'sam.image_encoder.blocks.4.mlp.lin2.bias', 'sam.image_encoder.blocks.5.norm1.weight', 'sam.image_encoder.blocks.5.norm1.bias', 'sam.image_encoder.blocks.5.attn.rel_pos_h', 'sam.image_encoder.blocks.5.attn.rel_pos_w', 'sam.image_encoder.blocks.5.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.5.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.5.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.5.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.5.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.5.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.5.attn.proj.weight', 'sam.image_encoder.blocks.5.attn.proj.bias', 'sam.image_encoder.blocks.5.norm2.weight', 'sam.image_encoder.blocks.5.norm2.bias', 'sam.image_encoder.blocks.5.mlp.lin1.weight', 'sam.image_encoder.blocks.5.mlp.lin1.bias', 'sam.image_encoder.blocks.5.mlp.lin2.weight', 'sam.image_encoder.blocks.5.mlp.lin2.bias', 'sam.image_encoder.blocks.6.norm1.weight', 'sam.image_encoder.blocks.6.norm1.bias', 'sam.image_encoder.blocks.6.attn.rel_pos_h', 'sam.image_encoder.blocks.6.attn.rel_pos_w', 'sam.image_encoder.blocks.6.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.6.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.6.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.6.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.6.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.6.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.6.attn.proj.weight', 'sam.image_encoder.blocks.6.attn.proj.bias', 'sam.image_encoder.blocks.6.norm2.weight', 'sam.image_encoder.blocks.6.norm2.bias', 'sam.image_encoder.blocks.6.mlp.lin1.weight', 'sam.image_encoder.blocks.6.mlp.lin1.bias', 'sam.image_encoder.blocks.6.mlp.lin2.weight', 'sam.image_encoder.blocks.6.mlp.lin2.bias', 'sam.image_encoder.blocks.7.norm1.weight', 'sam.image_encoder.blocks.7.norm1.bias', 'sam.image_encoder.blocks.7.attn.rel_pos_h', 'sam.image_encoder.blocks.7.attn.rel_pos_w', 'sam.image_encoder.blocks.7.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.7.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.7.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.7.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.7.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.7.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.7.attn.proj.weight', 'sam.image_encoder.blocks.7.attn.proj.bias', 'sam.image_encoder.blocks.7.norm2.weight', 'sam.image_encoder.blocks.7.norm2.bias', 'sam.image_encoder.blocks.7.mlp.lin1.weight', 'sam.image_encoder.blocks.7.mlp.lin1.bias', 'sam.image_encoder.blocks.7.mlp.lin2.weight', 'sam.image_encoder.blocks.7.mlp.lin2.bias', 'sam.image_encoder.blocks.8.norm1.weight', 'sam.image_encoder.blocks.8.norm1.bias', 'sam.image_encoder.blocks.8.attn.rel_pos_h', 'sam.image_encoder.blocks.8.attn.rel_pos_w', 'sam.image_encoder.blocks.8.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.8.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.8.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.8.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.8.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.8.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.8.attn.proj.weight', 'sam.image_encoder.blocks.8.attn.proj.bias', 'sam.image_encoder.blocks.8.norm2.weight', 'sam.image_encoder.blocks.8.norm2.bias', 'sam.image_encoder.blocks.8.mlp.lin1.weight', 'sam.image_encoder.blocks.8.mlp.lin1.bias', 'sam.image_encoder.blocks.8.mlp.lin2.weight', 'sam.image_encoder.blocks.8.mlp.lin2.bias', 'sam.image_encoder.blocks.9.norm1.weight', 'sam.image_encoder.blocks.9.norm1.bias', 'sam.image_encoder.blocks.9.attn.rel_pos_h', 'sam.image_encoder.blocks.9.attn.rel_pos_w', 'sam.image_encoder.blocks.9.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.9.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.9.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.9.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.9.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.9.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.9.attn.proj.weight', 'sam.image_encoder.blocks.9.attn.proj.bias', 'sam.image_encoder.blocks.9.norm2.weight', 'sam.image_encoder.blocks.9.norm2.bias', 'sam.image_encoder.blocks.9.mlp.lin1.weight', 'sam.image_encoder.blocks.9.mlp.lin1.bias', 'sam.image_encoder.blocks.9.mlp.lin2.weight', 'sam.image_encoder.blocks.9.mlp.lin2.bias', 'sam.image_encoder.blocks.10.norm1.weight', 'sam.image_encoder.blocks.10.norm1.bias', 'sam.image_encoder.blocks.10.attn.rel_pos_h', 'sam.image_encoder.blocks.10.attn.rel_pos_w', 'sam.image_encoder.blocks.10.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.10.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.10.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.10.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.10.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.10.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.10.attn.proj.weight', 'sam.image_encoder.blocks.10.attn.proj.bias', 'sam.image_encoder.blocks.10.norm2.weight', 'sam.image_encoder.blocks.10.norm2.bias', 'sam.image_encoder.blocks.10.mlp.lin1.weight', 'sam.image_encoder.blocks.10.mlp.lin1.bias', 'sam.image_encoder.blocks.10.mlp.lin2.weight', 'sam.image_encoder.blocks.10.mlp.lin2.bias', 'sam.image_encoder.blocks.11.norm1.weight', 'sam.image_encoder.blocks.11.norm1.bias', 'sam.image_encoder.blocks.11.attn.rel_pos_h', 'sam.image_encoder.blocks.11.attn.rel_pos_w', 'sam.image_encoder.blocks.11.attn.qkv.qkv.weight', 'sam.image_encoder.blocks.11.attn.qkv.qkv.bias', 'sam.image_encoder.blocks.11.attn.qkv.linear_a_q.weight', 'sam.image_encoder.blocks.11.attn.qkv.linear_b_q.weight', 'sam.image_encoder.blocks.11.attn.qkv.linear_a_v.weight', 'sam.image_encoder.blocks.11.attn.qkv.linear_b_v.weight', 'sam.image_encoder.blocks.11.attn.proj.weight', 'sam.image_encoder.blocks.11.attn.proj.bias', 'sam.image_encoder.blocks.11.norm2.weight', 'sam.image_encoder.blocks.11.norm2.bias', 'sam.image_encoder.blocks.11.mlp.lin1.weight', 'sam.image_encoder.blocks.11.mlp.lin1.bias', 'sam.image_encoder.blocks.11.mlp.lin2.weight', 'sam.image_encoder.blocks.11.mlp.lin2.bias', 'sam.image_encoder.neck.0.weight', 'sam.image_encoder.neck.1.weight', 'sam.image_encoder.neck.1.bias', 'sam.image_encoder.neck.2.weight', 'sam.image_encoder.neck.3.weight', 'sam.image_encoder.neck.3.bias', 'sam.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'sam.prompt_encoder.point_embeddings.0.weight', 'sam.prompt_encoder.point_embeddings.1.weight', 'sam.prompt_encoder.point_embeddings.2.weight', 'sam.prompt_encoder.point_embeddings.3.weight', 'sam.prompt_encoder.not_a_point_embed.weight', 'sam.prompt_encoder.mask_downscaling.0.weight', 'sam.prompt_encoder.mask_downscaling.0.bias', 'sam.prompt_encoder.mask_downscaling.1.weight', 'sam.prompt_encoder.mask_downscaling.1.bias', 'sam.prompt_encoder.mask_downscaling.3.weight', 'sam.prompt_encoder.mask_downscaling.3.bias', 'sam.prompt_encoder.mask_downscaling.4.weight', 'sam.prompt_encoder.mask_downscaling.4.bias', 'sam.prompt_encoder.mask_downscaling.6.weight', 'sam.prompt_encoder.mask_downscaling.6.bias', 'sam.prompt_encoder.no_mask_embed.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.self_attn.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'sam.mask_decoder.transformer.layers.0.self_attn.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.self_attn.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'sam.mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'sam.mask_decoder.transformer.layers.0.norm1.weight', 'sam.mask_decoder.transformer.layers.0.norm1.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'sam.mask_decoder.transformer.layers.0.norm2.weight', 'sam.mask_decoder.transformer.layers.0.norm2.bias', 'sam.mask_decoder.transformer.layers.0.mlp.lin1.weight', 'sam.mask_decoder.transformer.layers.0.mlp.lin1.bias', 'sam.mask_decoder.transformer.layers.0.mlp.lin2.weight', 'sam.mask_decoder.transformer.layers.0.mlp.lin2.bias', 'sam.mask_decoder.transformer.layers.0.norm3.weight', 'sam.mask_decoder.transformer.layers.0.norm3.bias', 'sam.mask_decoder.transformer.layers.0.norm4.weight', 'sam.mask_decoder.transformer.layers.0.norm4.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'sam.mask_decoder.transformer.layers.1.self_attn.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.self_attn.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'sam.mask_decoder.transformer.layers.1.self_attn.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.self_attn.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'sam.mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'sam.mask_decoder.transformer.layers.1.norm1.weight', 'sam.mask_decoder.transformer.layers.1.norm1.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'sam.mask_decoder.transformer.layers.1.norm2.weight', 'sam.mask_decoder.transformer.layers.1.norm2.bias', 'sam.mask_decoder.transformer.layers.1.mlp.lin1.weight', 'sam.mask_decoder.transformer.layers.1.mlp.lin1.bias', 'sam.mask_decoder.transformer.layers.1.mlp.lin2.weight', 'sam.mask_decoder.transformer.layers.1.mlp.lin2.bias', 'sam.mask_decoder.transformer.layers.1.norm3.weight', 'sam.mask_decoder.transformer.layers.1.norm3.bias', 'sam.mask_decoder.transformer.layers.1.norm4.weight', 'sam.mask_decoder.transformer.layers.1.norm4.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.proj.bias', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.w_a.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.w_b.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.proj.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.proj.bias', 'sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.w_a.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.w_b.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.proj.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.proj.bias', 'sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.w_a.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.w_b.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'sam.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'sam.mask_decoder.transformer.norm_final_attn.weight', 'sam.mask_decoder.transformer.norm_final_attn.bias', 'sam.mask_decoder.iou_token.weight', 'sam.mask_decoder.mask_tokens.weight', 'sam.mask_decoder.output_upscaling.0.weight', 'sam.mask_decoder.output_upscaling.0.bias', 'sam.mask_decoder.output_upscaling.1.weight', 'sam.mask_decoder.output_upscaling.1.bias', 'sam.mask_decoder.output_upscaling.3.weight', 'sam.mask_decoder.output_upscaling.3.bias', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.4.layers.2.bias', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.0.weight', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.0.bias', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.1.weight', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.1.bias', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.2.weight', 'sam.mask_decoder.output_hypernetworks_mlps.5.layers.2.bias', 'sam.mask_decoder.iou_prediction_head.layers.0.weight', 'sam.mask_decoder.iou_prediction_head.layers.0.bias', 'sam.mask_decoder.iou_prediction_head.layers.1.weight', 'sam.mask_decoder.iou_prediction_head.layers.1.bias', 'sam.mask_decoder.iou_prediction_head.layers.2.weight', 'sam.mask_decoder.iou_prediction_head.layers.2.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the file\n",
    "checkpoint = torch.load('../bestweights_samed_512_r6_augment_no_normalize_yes.pt')\n",
    "\n",
    "# Check the keys to understand the structure\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fa_ti_q_proj_A.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: fa_ti_q_proj_B.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: fa_ti_v_proj_A.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: fa_ti_v_proj_B.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.image_encoder.pos_embed | Weights Shape: torch.Size([1, 32, 32, 768])\n",
      "Layer: sam.image_encoder.patch_embed.proj.weight | Weights Shape: torch.Size([768, 3, 16, 16])\n",
      "Layer: sam.image_encoder.patch_embed.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.0.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.0.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.0.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.0.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.0.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.0.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.0.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.0.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.1.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.1.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.1.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.1.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.1.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.1.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.1.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.1.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.attn.rel_pos_h | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.2.attn.rel_pos_w | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.2.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.2.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.2.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.2.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.2.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.2.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.2.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.3.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.3.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.3.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.3.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.3.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.3.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.3.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.3.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.4.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.4.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.4.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.4.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.4.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.4.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.4.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.4.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.attn.rel_pos_h | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.5.attn.rel_pos_w | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.5.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.5.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.5.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.5.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.5.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.5.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.5.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.6.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.6.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.6.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.6.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.6.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.6.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.6.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.6.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.7.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.7.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.7.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.7.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.7.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.7.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.7.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.7.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.attn.rel_pos_h | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.8.attn.rel_pos_w | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.8.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.8.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.8.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.8.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.8.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.8.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.8.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.9.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.9.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.9.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.9.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.9.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.9.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.9.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.9.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.attn.rel_pos_h | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.10.attn.rel_pos_w | Weights Shape: torch.Size([27, 64])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.10.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.10.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.10.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.10.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.10.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.10.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.10.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.norm1.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.norm1.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.attn.rel_pos_h | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.11.attn.rel_pos_w | Weights Shape: torch.Size([63, 64])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.qkv.weight | Weights Shape: torch.Size([2304, 768])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.qkv.bias | Weights Shape: torch.Size([2304])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.linear_a_q.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.linear_b_q.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.linear_a_v.weight | Weights Shape: torch.Size([6, 768])\n",
      "Layer: sam.image_encoder.blocks.11.attn.qkv.linear_b_v.weight | Weights Shape: torch.Size([768, 6])\n",
      "Layer: sam.image_encoder.blocks.11.attn.proj.weight | Weights Shape: torch.Size([768, 768])\n",
      "Layer: sam.image_encoder.blocks.11.attn.proj.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.norm2.weight | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.norm2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.blocks.11.mlp.lin1.weight | Weights Shape: torch.Size([3072, 768])\n",
      "Layer: sam.image_encoder.blocks.11.mlp.lin1.bias | Weights Shape: torch.Size([3072])\n",
      "Layer: sam.image_encoder.blocks.11.mlp.lin2.weight | Weights Shape: torch.Size([768, 3072])\n",
      "Layer: sam.image_encoder.blocks.11.mlp.lin2.bias | Weights Shape: torch.Size([768])\n",
      "Layer: sam.image_encoder.neck.0.weight | Weights Shape: torch.Size([256, 768, 1, 1])\n",
      "Layer: sam.image_encoder.neck.1.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.image_encoder.neck.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.image_encoder.neck.2.weight | Weights Shape: torch.Size([256, 256, 3, 3])\n",
      "Layer: sam.image_encoder.neck.3.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.image_encoder.neck.3.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix | Weights Shape: torch.Size([2, 128])\n",
      "Layer: sam.prompt_encoder.point_embeddings.0.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.prompt_encoder.point_embeddings.1.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.prompt_encoder.point_embeddings.2.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.prompt_encoder.point_embeddings.3.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.prompt_encoder.not_a_point_embed.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.0.weight | Weights Shape: torch.Size([4, 1, 2, 2])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.0.bias | Weights Shape: torch.Size([4])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.1.weight | Weights Shape: torch.Size([4])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.1.bias | Weights Shape: torch.Size([4])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.3.weight | Weights Shape: torch.Size([16, 4, 2, 2])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.3.bias | Weights Shape: torch.Size([16])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.4.weight | Weights Shape: torch.Size([16])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.4.bias | Weights Shape: torch.Size([16])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.6.weight | Weights Shape: torch.Size([256, 16, 1, 1])\n",
      "Layer: sam.prompt_encoder.mask_downscaling.6.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.prompt_encoder.no_mask_embed.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.q_proj.proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.q_proj.proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.q_proj.w_b.weight | Weights Shape: torch.Size([256, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.k_proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.k_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.v_proj.proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.v_proj.proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.v_proj.w_b.weight | Weights Shape: torch.Size([256, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.out_proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.self_attn.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm1.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight | Weights Shape: torch.Size([256, 128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm2.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm2.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.mlp.lin1.weight | Weights Shape: torch.Size([2048, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.mlp.lin1.bias | Weights Shape: torch.Size([2048])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.mlp.lin2.weight | Weights Shape: torch.Size([256, 2048])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.mlp.lin2.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm3.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm3.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm4.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.norm4.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight | Weights Shape: torch.Size([256, 128])\n",
      "Layer: sam.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.q_proj.proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.q_proj.proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.q_proj.w_b.weight | Weights Shape: torch.Size([256, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.k_proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.k_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.v_proj.proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.v_proj.proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.v_proj.w_b.weight | Weights Shape: torch.Size([256, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.out_proj.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.self_attn.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm1.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight | Weights Shape: torch.Size([256, 128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm2.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm2.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.mlp.lin1.weight | Weights Shape: torch.Size([2048, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.mlp.lin1.bias | Weights Shape: torch.Size([2048])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.mlp.lin2.weight | Weights Shape: torch.Size([256, 2048])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.mlp.lin2.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm3.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm3.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm4.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.norm4.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight | Weights Shape: torch.Size([256, 128])\n",
      "Layer: sam.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.q_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.proj.weight | Weights Shape: torch.Size([128, 256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.proj.bias | Weights Shape: torch.Size([128])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.w_a.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.v_proj.w_b.weight | Weights Shape: torch.Size([128, 6])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight | Weights Shape: torch.Size([256, 128])\n",
      "Layer: sam.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.norm_final_attn.weight | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.transformer.norm_final_attn.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.iou_token.weight | Weights Shape: torch.Size([1, 256])\n",
      "Layer: sam.mask_decoder.mask_tokens.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.output_upscaling.0.weight | Weights Shape: torch.Size([256, 64, 2, 2])\n",
      "Layer: sam.mask_decoder.output_upscaling.0.bias | Weights Shape: torch.Size([64])\n",
      "Layer: sam.mask_decoder.output_upscaling.1.weight | Weights Shape: torch.Size([64])\n",
      "Layer: sam.mask_decoder.output_upscaling.1.bias | Weights Shape: torch.Size([64])\n",
      "Layer: sam.mask_decoder.output_upscaling.3.weight | Weights Shape: torch.Size([64, 32, 2, 2])\n",
      "Layer: sam.mask_decoder.output_upscaling.3.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.4.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.2.weight | Weights Shape: torch.Size([32, 256])\n",
      "Layer: sam.mask_decoder.output_hypernetworks_mlps.5.layers.2.bias | Weights Shape: torch.Size([32])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.0.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.0.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.1.weight | Weights Shape: torch.Size([256, 256])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.1.bias | Weights Shape: torch.Size([256])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.2.weight | Weights Shape: torch.Size([6, 256])\n",
      "Layer: sam.mask_decoder.iou_prediction_head.layers.2.bias | Weights Shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "for layer, weights in state_dict.items():\n",
    "    print(f\"Layer: {layer} | Weights Shape: {weights.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your model has a convolutional layer, for example 'conv1.weight'\n",
    "# For a fully connected layer, for example 'fc.weight'\n",
    "fc_weights = state_dict['sam.mask_decoder.iou_prediction_head.layers.2.weight'].cpu().numpy()\n",
    "\n",
    "plt.hist(fc_weights.flatten(), bins=100)\n",
    "plt.title('Fully Connected Layer Weights Distribution')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
