[![](https://img.shields.io/badge/PyTorch-2.4.0-ee4c2c.svg)](https://pytorch.org/) [![](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)  [![](https://img.shields.io/github/license/ResidentMario/missingno.svg)](https://github.com/WouterBant/AI4MI/blob/main/LICENSE) 

# CT scan image segmentation: nnUnet vs finetuning SAM 

<table align="center">
  <tr align="center">
      <td><img src="assets/figure1.webp" width="60%"></td>
  </tr>
  <tr align="center">
      <td><em>Image generated by DALL-E 2</em></td>
  </tr>
</table>

## Introduction
The code for reproducing our paper "". We experiment with different SAM based methods and more standard methods like Enet and nnUnet on the SEGTHOR dataset. Short description of our main findings. The obtained results can be found [here](results) and can be converted to human unreadable npy files with [csv2npy.py](results_metrics/csv2npy.py).

## Quick Start

### Installation instructions

Get the code:
```bash
git clone https://github.com/WouterBant/AI4MI.git
```

We use conda for package management, create an environment with:
```bash
conda env create -f env.yml
```

Activate this environment to run the code without dependency problems:
```bash
conda activate ai4mi
```

### Getting the augmentations
First install batchaugmenters:
```pip install --upgrade batchgenerators```

To generate the augmentations with the same parameters and probabilities as nnUNet run:

```python data_augmenter.py```

This takes the images from ```data/SEGTHOR_MANUAL_SPLIT/train```, augments some of them and saves these again to this folder, resulting in all original images + augmented images.

If you want to augment images from a different folder or same them to a different folder you can do that through the command line arguments.


### Getting the pretrained checkpoints
It is possible to obtain the checkpoints of all models of which results are presented in the paper. [checkpoints/download_checkpoints.sh](checkpoints/download_checkpoints.sh) contains commands to download the various models. By default all commands are commented, uncomment the ones you are interested and run:

```bash
bash checkpoints/download_checkpoints.sh
```

For future reference: when training a model with `torch.compile` save models with [`torch.save(getattr(model, '_orig_mod', model).state_dict(), "best_weights.pt")`](https://github.com/pytorch/pytorch/issues/101107). Now we removed the _orig_mod prefixes with [fix_checkpoints.py](checkpoints/fix_checkpoints.py). This was done to be able to load models without compiling (which is not possible on CPU and the overhead of JIT outweighs its performance increase for inference on small datasets).

### Running the code

Run training with:
```bash
python src/main.py
```

When using the optional `wandb` flag make sure to export your api key with:

```bash
export WANDB_API_KEY="Your key here"
```

<details> <summary>Click to expand full usage </summary>

```bash
usage: main.py [-h] [--epochs EPOCHS] [--lr LR] [--weight_decay WEIGHT_DECAY] [--batch_size BATCH_SIZE] [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] [--use_scheduler] [--use_sampler] [--augment] [--model MODEL] [--optimizer {adam,sgd,adamw,sgd-wd}]
               [--dataset {TOY2,SEGTHOR,SEGTHOR_MANUAL_SPLIT}] [--mode {partial,full}] [--loss {ce,dice_monai,gdl,dce}] [--ce_lambda CE_LAMBDA] [--dest DEST] [--r R] [--from_checkpoint FROM_CHECKPOINT] [--gpu] [--num_workers NUM_WORKERS] [--debug] [--normalize] [--deterministic]
               [--use_wandb] [--clip_grad] [--crf] [--finetune_crf]

options:
  -h, --help            show this help message and exit
  --epochs EPOCHS
  --lr LR               Learning rate
  --weight_decay WEIGHT_DECAY
                        Weight decay
  --batch_size BATCH_SIZE
                        Batch size
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of steps to accumulate gradients over
  --use_scheduler       Use CosineWarmupScheduler
  --use_sampler         Use AdaptiveSampler
  --augment             Augment the training dataset
  --model MODEL         Model to use
  --optimizer {adam,sgd,adamw,sgd-wd}
                        Optimizer to use
  --dataset {TOY2,SEGTHOR,SEGTHOR_MANUAL_SPLIT}
  --mode {partial,full}
  --loss {ce,dice_monai,gdl,dce}
  --ce_lambda CE_LAMBDA
  --dest DEST           Destination directory to save the results (predictions and weights).
  --r R                 The rank of the LoRa matrices.
  --from_checkpoint FROM_CHECKPOINT
  --gpu
  --num_workers NUM_WORKERS
  --debug               Keep only a fraction (10 samples) of the datasets, to test the logic around epochs and logging easily.
  --normalize           Normalize the input images
  --deterministic       Make the training deterministic
  --use_wandb           Use wandb for logging
  --clip_grad           Enable gradient clipping
  --crf                 Apply CRF on the output
  --finetune_crf        Freeze the model and only train CRF and the last layer
```
</details>

<br>

And evaluation with:

```bash
python src/test.py --from_checkpoint [your_checkpoint]
```

Or easier (uncomment the lines of models you want to test and make sure to download the corresponding checkpoint first):
```bash
bash test.sh
```

<details> <summary>Click to expand full usage </summary>

<br>

Note that for the compiled models a GPU is required to run inference as the model needs to be comiled again.

```bash
usage: test.py [-h] [--batch_size BATCH_SIZE] [--model MODEL] [--dest DEST] [--r R] [--dataset {TOY2,SEGTHOR,SEGTHOR_MANUAL_SPLIT}] [--mode {partial,full}] [--from_checkpoint FROM_CHECKPOINT] [--gpu] [--num_workers NUM_WORKERS] [--debug] [--normalize]

options:
  -h, --help            show this help message and exit
  --batch_size BATCH_SIZE
                        Batch size
  --model MODEL         Model to use
  --dest DEST           Destination directory to save the results (predictions and weights).
  --r R                 The rank of the LoRa matrices.
  --dataset {TOY2,SEGTHOR,SEGTHOR_MANUAL_SPLIT}
  --mode {partial,full}
  --from_checkpoint FROM_CHECKPOINT
  --gpu
  --num_workers NUM_WORKERS
  --debug               Keep only a fraction (10 samples) of the datasets, to test the logic around epochs and logging easily.
  --normalize           Normalize the input images
```
</details>


## Our contributions
- Notebook showing how we were able to fix the data (with and without the provided transformation matrix) and are able to work with nifti files: [notebooks/heart_transform](notebooks/heart_transform.ipynb), this is incorporated in [src/slice_seghtor.py](src/slice_segthor.py).
- Implementation of by us chosen metrics: [src/metrics.py](src/metrics.py).
- [Inference](src/test.py).
- [Our version of SAMed](src/samed/), the most imporatant change is using `masks` instead of `low_res_logits` in the [training loop](src/main.py).
- [Cosine learning rate scheduler](src/scheduler.py).
- [CRF wrapper class for models](src/crf_model.py).
- [Adaptive sampler to downsample only background images](src/adaptive_sampler.py).
- [Wandb integration with uploading predicted segmentations during training](src/utils.py).
- Notebooks for [interpretability](notebooks/interpretability.ipynb) (and its [code](notebooks/notebook_utils.py)), [image normalization visualization](notebooks/normalize.ipynb), [augmentation visualization](notebooks/augmentations.ipynb), and [data analysis](notebooks/data_analysis.ipynb) can be found in the [notebooks](notebooks) folder.


## Acknowledgements
This was part of a project for the course AI for Medical Imaging (2024) at the University of Amsterdam. Some base code was provided: https://github.com/HKervadec/ai4mi_project. 

- SAM: https://github.com/facebookresearch/segment-an
- Implementation for LoRA for SAM: https://github.com/JamesQFreeman/Sam_LoRA
- We further adopted parts of the code of samed who also fine-tuned SAM for medical image segmentation: https://github.com/hitachinsk/SAMed

We started of from https://github.com/JamesQFreeman/Sam_LoRA and committed changes from there on. Note that some of these changes come directly from https://github.com/hitachinsk/SAMed.
